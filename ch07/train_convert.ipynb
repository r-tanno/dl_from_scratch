{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "opti='RMSprop'\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer=opti, optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2742897185847135\n",
      "=== epoch:1, train acc:0.237, test acc:0.237 ===\n",
      "train loss:2.0755352709580217\n",
      "train loss:1.8108659052575249\n",
      "train loss:1.6372456779071884\n",
      "train loss:1.712396215983947\n",
      "train loss:1.2578373739042852\n",
      "train loss:1.10054962220269\n",
      "train loss:1.1080182130794793\n",
      "train loss:1.2104489128691087\n",
      "train loss:0.828311631374026\n",
      "train loss:0.7849868994524463\n",
      "train loss:0.7963069084492135\n",
      "train loss:0.616519273530875\n",
      "train loss:0.6888839986377212\n",
      "train loss:0.7915524784176681\n",
      "train loss:0.750906973742252\n",
      "train loss:0.741586405132094\n",
      "train loss:0.5340016003765442\n",
      "train loss:0.5846033065712174\n",
      "train loss:0.42934730502404383\n",
      "train loss:0.4870979940187776\n",
      "train loss:0.46874266575881185\n",
      "train loss:0.33729866999444963\n",
      "train loss:0.36496405522844716\n",
      "train loss:0.4527314531477999\n",
      "train loss:0.41384186782353977\n",
      "train loss:0.595011941697425\n",
      "train loss:0.43931472006097105\n",
      "train loss:0.27822641232306267\n",
      "train loss:0.2654544336749142\n",
      "train loss:0.27941997534768936\n",
      "train loss:0.5703574955036108\n",
      "train loss:0.38170918633866613\n",
      "train loss:0.47786412288051755\n",
      "train loss:0.3027829199078978\n",
      "train loss:0.3689815457587078\n",
      "train loss:0.352237410537416\n",
      "train loss:0.3193709160921712\n",
      "train loss:0.28937975878949723\n",
      "train loss:0.26442713254489997\n",
      "train loss:0.19139897310204465\n",
      "train loss:0.2275280939590408\n",
      "train loss:0.38790087410928364\n",
      "train loss:0.2824241675907901\n",
      "train loss:0.25450063339750917\n",
      "train loss:0.19447411178385676\n",
      "train loss:0.3671162487988684\n",
      "train loss:0.2954289458074996\n",
      "train loss:0.183431871984128\n",
      "train loss:0.3681957389285161\n",
      "train loss:0.23002383696540515\n",
      "=== epoch:2, train acc:0.886, test acc:0.861 ===\n",
      "train loss:0.317392753600689\n",
      "train loss:0.3300270415711343\n",
      "train loss:0.4374365059804057\n",
      "train loss:0.22230360668639684\n",
      "train loss:0.22235144367002532\n",
      "train loss:0.352186777241345\n",
      "train loss:0.213362521315474\n",
      "train loss:0.27414280310058614\n",
      "train loss:0.20984459152840734\n",
      "train loss:0.19690716027251437\n",
      "train loss:0.17597446036175005\n",
      "train loss:0.18849247559510868\n",
      "train loss:0.24603394645619062\n",
      "train loss:0.19152351063400286\n",
      "train loss:0.19208300361414754\n",
      "train loss:0.22664852404046518\n",
      "train loss:0.23032794394423042\n",
      "train loss:0.16871640610679794\n",
      "train loss:0.25294002388675096\n",
      "train loss:0.1169030332343521\n",
      "train loss:0.2262539792963095\n",
      "train loss:0.29875199329247965\n",
      "train loss:0.11094836378048635\n",
      "train loss:0.1652968966812045\n",
      "train loss:0.2136954450786474\n",
      "train loss:0.15330144012970146\n",
      "train loss:0.2208760346792064\n",
      "train loss:0.15565002284386303\n",
      "train loss:0.10477008593336037\n",
      "train loss:0.18685518442214902\n",
      "train loss:0.12773141680188388\n",
      "train loss:0.15828782019833917\n",
      "train loss:0.1090582776129665\n",
      "train loss:0.22340182923636967\n",
      "train loss:0.27876070029056654\n",
      "train loss:0.28289142072612306\n",
      "train loss:0.1688632271753635\n",
      "train loss:0.1351527542225344\n",
      "train loss:0.18561921346267507\n",
      "train loss:0.22483118309365097\n",
      "train loss:0.16308897589023588\n",
      "train loss:0.0666795416607142\n",
      "train loss:0.1655547023837013\n",
      "train loss:0.3243335491992253\n",
      "train loss:0.20636924603877865\n",
      "train loss:0.08505665471690409\n",
      "train loss:0.14757318730555524\n",
      "train loss:0.16061121798266256\n",
      "train loss:0.27772373891273633\n",
      "train loss:0.1938166040058693\n",
      "=== epoch:3, train acc:0.904, test acc:0.907 ===\n",
      "train loss:0.09983434006188602\n",
      "train loss:0.12959584834885265\n",
      "train loss:0.14813552854191703\n",
      "train loss:0.13731596042697228\n",
      "train loss:0.16319526355926187\n",
      "train loss:0.12120752815371551\n",
      "train loss:0.11904641364248877\n",
      "train loss:0.1663755486863133\n",
      "train loss:0.21772132390530782\n",
      "train loss:0.11646677981506459\n",
      "train loss:0.1283042546090649\n",
      "train loss:0.1329159631717295\n",
      "train loss:0.0988488210137664\n",
      "train loss:0.26221015779519324\n",
      "train loss:0.20760533380029766\n",
      "train loss:0.07363557611518666\n",
      "train loss:0.19474904688296749\n",
      "train loss:0.16034285747953306\n",
      "train loss:0.1766540169641081\n",
      "train loss:0.09047172302790576\n",
      "train loss:0.13333376763941962\n",
      "train loss:0.11869752072370834\n",
      "train loss:0.09321354217660954\n",
      "train loss:0.10176983702790693\n",
      "train loss:0.05957160321690076\n",
      "train loss:0.1520972947089503\n",
      "train loss:0.1075078783844794\n",
      "train loss:0.09703659676573315\n",
      "train loss:0.07517027814759887\n",
      "train loss:0.06468846679696595\n",
      "train loss:0.10998316595609571\n",
      "train loss:0.12406893824577558\n",
      "train loss:0.09399161958928515\n",
      "train loss:0.10790809997598587\n",
      "train loss:0.28666701262672095\n",
      "train loss:0.1996927808840341\n",
      "train loss:0.1877853570551221\n",
      "train loss:0.11399310644633907\n",
      "train loss:0.13958344558412836\n",
      "train loss:0.06883798819755721\n",
      "train loss:0.14651491273062944\n",
      "train loss:0.14849497939116613\n",
      "train loss:0.22753595616547084\n",
      "train loss:0.12440843592631678\n",
      "train loss:0.06586038971530163\n",
      "train loss:0.0886043004137012\n",
      "train loss:0.13717422780671623\n",
      "train loss:0.08578513260454798\n",
      "train loss:0.11173491918209047\n",
      "train loss:0.1714931437515279\n",
      "=== epoch:4, train acc:0.938, test acc:0.93 ===\n",
      "train loss:0.05866634084186286\n",
      "train loss:0.09239435039920116\n",
      "train loss:0.12724048604583454\n",
      "train loss:0.05939320235724971\n",
      "train loss:0.23216460985081308\n",
      "train loss:0.1302677731004024\n",
      "train loss:0.11952799231081396\n",
      "train loss:0.09511741459953665\n",
      "train loss:0.13268391925933787\n",
      "train loss:0.15498836508388147\n",
      "train loss:0.1809949999807072\n",
      "train loss:0.14056224824316163\n",
      "train loss:0.07934362466421022\n",
      "train loss:0.05497194820323552\n",
      "train loss:0.1017963334951044\n",
      "train loss:0.09527416857943959\n",
      "train loss:0.11556551269446269\n",
      "train loss:0.10559491495320775\n",
      "train loss:0.052815514717216426\n",
      "train loss:0.03879726693214701\n",
      "train loss:0.11366666044183807\n",
      "train loss:0.07370318896377423\n",
      "train loss:0.046803040076184715\n",
      "train loss:0.06426648634935359\n",
      "train loss:0.09405669976586756\n",
      "train loss:0.050224192247296025\n",
      "train loss:0.1636877171902356\n",
      "train loss:0.08804379838328373\n",
      "train loss:0.060978935303507856\n",
      "train loss:0.14344012542021872\n",
      "train loss:0.07573019829051154\n",
      "train loss:0.14385392834305338\n",
      "train loss:0.08380332922594974\n",
      "train loss:0.0888446924256865\n",
      "train loss:0.08723023884286629\n",
      "train loss:0.1747291514125211\n",
      "train loss:0.14734200312787446\n",
      "train loss:0.040869257303342274\n",
      "train loss:0.08121711203505619\n",
      "train loss:0.11128870467773332\n",
      "train loss:0.15150738975903125\n",
      "train loss:0.0764633977451243\n",
      "train loss:0.07252437638762733\n",
      "train loss:0.1141333427074081\n",
      "train loss:0.11125468339801385\n",
      "train loss:0.08970179342967263\n",
      "train loss:0.06028591361829478\n",
      "train loss:0.12109679552981145\n",
      "train loss:0.12024851877062696\n",
      "train loss:0.07231898437695689\n",
      "=== epoch:5, train acc:0.948, test acc:0.936 ===\n",
      "train loss:0.07048349470692777\n",
      "train loss:0.05700486120652592\n",
      "train loss:0.16881673399968006\n",
      "train loss:0.05303466586354138\n",
      "train loss:0.0872437010095534\n",
      "train loss:0.07957931715630785\n",
      "train loss:0.12089725507090308\n",
      "train loss:0.059423326096424714\n",
      "train loss:0.03653504553721929\n",
      "train loss:0.054366450595711366\n",
      "train loss:0.08936693805368533\n",
      "train loss:0.10072579923548924\n",
      "train loss:0.10113810655956156\n",
      "train loss:0.07212861008138204\n",
      "train loss:0.046275744452754654\n",
      "train loss:0.038240544913345303\n",
      "train loss:0.14771556404151884\n",
      "train loss:0.03655187718423997\n",
      "train loss:0.07934414443178485\n",
      "train loss:0.06683319074140961\n",
      "train loss:0.051019593910010146\n",
      "train loss:0.11589546284072866\n",
      "train loss:0.06401513387845659\n",
      "train loss:0.041227056165462204\n",
      "train loss:0.04875217678007845\n",
      "train loss:0.09567036699322357\n",
      "train loss:0.061543825777396394\n",
      "train loss:0.1251113179370004\n",
      "train loss:0.07769340363944878\n",
      "train loss:0.07810358364960616\n",
      "train loss:0.07444835662705704\n",
      "train loss:0.04356169883083685\n",
      "train loss:0.0928439319969367\n",
      "train loss:0.08946284815176214\n",
      "train loss:0.043670198079444535\n",
      "train loss:0.06695534435285802\n",
      "train loss:0.07154874488183133\n",
      "train loss:0.054418670835034655\n",
      "train loss:0.02694362441528145\n",
      "train loss:0.054690814477870664\n",
      "train loss:0.0880433517404407\n",
      "train loss:0.05158618863772721\n",
      "train loss:0.048914297168695525\n",
      "train loss:0.10689358002957684\n",
      "train loss:0.04434041823111144\n",
      "train loss:0.08210011692108646\n",
      "train loss:0.03427135495711063\n",
      "train loss:0.14818022073717355\n",
      "train loss:0.06779210692109273\n",
      "train loss:0.054901187093492974\n",
      "=== epoch:6, train acc:0.951, test acc:0.944 ===\n",
      "train loss:0.03265414949553838\n",
      "train loss:0.04018203764782534\n",
      "train loss:0.0581851676784677\n",
      "train loss:0.07488724979177012\n",
      "train loss:0.04572155263841374\n",
      "train loss:0.0722354443289255\n",
      "train loss:0.08534469404341241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.1254134919452327\n",
      "train loss:0.03194857580941106\n",
      "train loss:0.08804626218846119\n",
      "train loss:0.03390234069183233\n",
      "train loss:0.053401264521164074\n",
      "train loss:0.0377802952836314\n",
      "train loss:0.08170385303437694\n",
      "train loss:0.09645458431593833\n",
      "train loss:0.05025900027654745\n",
      "train loss:0.12908530816140376\n",
      "train loss:0.06125528151969518\n",
      "train loss:0.06427850949779329\n",
      "train loss:0.04214510820820047\n",
      "train loss:0.08384460143014923\n",
      "train loss:0.17093446498605178\n",
      "train loss:0.02701694777344503\n",
      "train loss:0.0530996775418452\n",
      "train loss:0.059306868730565225\n",
      "train loss:0.04738403325582917\n",
      "train loss:0.047883149937973955\n",
      "train loss:0.07992465604943248\n",
      "train loss:0.03881453680087354\n",
      "train loss:0.06510727444403311\n",
      "train loss:0.09982145724217116\n",
      "train loss:0.05707411874176275\n",
      "train loss:0.044530543097797165\n",
      "train loss:0.1026328606399383\n",
      "train loss:0.04494980778498894\n",
      "train loss:0.025920533653412892\n",
      "train loss:0.0557242942481476\n",
      "train loss:0.08272802417955759\n",
      "train loss:0.04216850950408833\n",
      "train loss:0.0736934457075907\n",
      "train loss:0.07168100510672966\n",
      "train loss:0.03446296420355342\n",
      "train loss:0.06100124290545229\n",
      "train loss:0.022499703383805664\n",
      "train loss:0.03542305057965116\n",
      "train loss:0.04936021288780673\n",
      "train loss:0.031032742703020585\n",
      "train loss:0.02369692399051146\n",
      "train loss:0.10756099509728609\n",
      "train loss:0.04255462530231428\n",
      "=== epoch:7, train acc:0.961, test acc:0.948 ===\n",
      "train loss:0.03204270876364999\n",
      "train loss:0.02946606303845672\n",
      "train loss:0.045153574984509615\n",
      "train loss:0.05880100231722127\n",
      "train loss:0.029841072718617162\n",
      "train loss:0.028267932038317414\n",
      "train loss:0.09474542882033937\n",
      "train loss:0.01325763855887148\n",
      "train loss:0.031216871223466326\n",
      "train loss:0.044104343549526755\n",
      "train loss:0.02984743687361918\n",
      "train loss:0.03817070220964446\n",
      "train loss:0.02924433356343147\n",
      "train loss:0.04730363777390498\n",
      "train loss:0.030806825994397234\n",
      "train loss:0.06544484989468317\n",
      "train loss:0.09293433589501206\n",
      "train loss:0.047095070617423004\n",
      "train loss:0.038572273558143494\n",
      "train loss:0.03016341242803817\n",
      "train loss:0.0762420561199628\n",
      "train loss:0.020009035682416597\n",
      "train loss:0.05622989333183097\n",
      "train loss:0.02290081969806812\n",
      "train loss:0.0518915963736184\n",
      "train loss:0.04426474433994195\n",
      "train loss:0.031393329845651435\n",
      "train loss:0.027120196597958845\n",
      "train loss:0.044615726812968964\n",
      "train loss:0.1069458668353252\n",
      "train loss:0.0402008585087093\n",
      "train loss:0.038125414721105255\n",
      "train loss:0.04082038073691518\n",
      "train loss:0.03395676920009269\n",
      "train loss:0.024286933043468198\n",
      "train loss:0.025417470629424975\n",
      "train loss:0.025201290178662846\n",
      "train loss:0.0350843945352278\n",
      "train loss:0.04090572349231322\n",
      "train loss:0.03141235690020723\n",
      "train loss:0.022814635137426494\n",
      "train loss:0.019259598469386158\n",
      "train loss:0.027034214092773776\n",
      "train loss:0.022885791114691544\n",
      "train loss:0.018716270579783038\n",
      "train loss:0.05065598692772397\n",
      "train loss:0.015917520924179508\n",
      "train loss:0.011621277764938607\n",
      "train loss:0.021875317609422323\n",
      "train loss:0.10051483457392317\n",
      "=== epoch:8, train acc:0.976, test acc:0.947 ===\n",
      "train loss:0.042367639125662826\n",
      "train loss:0.03325503106751422\n",
      "train loss:0.05585685467224941\n",
      "train loss:0.027683852515832913\n",
      "train loss:0.055787190209913955\n",
      "train loss:0.02859648069059143\n",
      "train loss:0.020059593735474442\n",
      "train loss:0.03047316814672504\n",
      "train loss:0.04790214791287429\n",
      "train loss:0.023167815795112026\n",
      "train loss:0.06498724362186324\n",
      "train loss:0.02478749400358718\n",
      "train loss:0.01676614512144736\n",
      "train loss:0.026009253129692683\n",
      "train loss:0.017670047558086183\n",
      "train loss:0.03686913893378528\n",
      "train loss:0.015726214611651884\n",
      "train loss:0.017268689483095175\n",
      "train loss:0.0663654100526354\n",
      "train loss:0.03127877719725991\n",
      "train loss:0.04664659088581691\n",
      "train loss:0.037780959615960216\n",
      "train loss:0.0276604188934358\n",
      "train loss:0.048913939770431784\n",
      "train loss:0.04800684940768747\n",
      "train loss:0.024190072166075972\n",
      "train loss:0.06313790678971325\n",
      "train loss:0.033069257934537494\n",
      "train loss:0.04732842749448825\n",
      "train loss:0.03689218973968592\n",
      "train loss:0.036931628483009224\n",
      "train loss:0.030555199011087192\n",
      "train loss:0.031671439802840325\n",
      "train loss:0.020054327678174246\n",
      "train loss:0.02606836440071622\n",
      "train loss:0.026289258591854808\n",
      "train loss:0.05329829701481011\n",
      "train loss:0.03392170028234929\n",
      "train loss:0.016551826438836428\n",
      "train loss:0.02182662817632749\n",
      "train loss:0.02904845864974992\n",
      "train loss:0.02163671348913617\n",
      "train loss:0.048851028779372585\n",
      "train loss:0.022155480907982313\n",
      "train loss:0.018915649425127537\n",
      "train loss:0.018637492640395374\n",
      "train loss:0.050036501395903724\n",
      "train loss:0.03356604970094416\n",
      "train loss:0.03276134247507722\n",
      "train loss:0.02138634872689676\n",
      "=== epoch:9, train acc:0.979, test acc:0.96 ===\n",
      "train loss:0.018009185152786197\n",
      "train loss:0.07653162810671095\n",
      "train loss:0.025609948624573074\n",
      "train loss:0.08004583629915649\n",
      "train loss:0.01759004941234737\n",
      "train loss:0.04238546809177404\n",
      "train loss:0.016827787946903438\n",
      "train loss:0.026064208162847192\n",
      "train loss:0.01964303858967988\n",
      "train loss:0.03995568943428154\n",
      "train loss:0.06420973887406775\n",
      "train loss:0.022197383555042048\n",
      "train loss:0.023473454704071015\n",
      "train loss:0.04041992864484761\n",
      "train loss:0.025017168773256923\n",
      "train loss:0.03816056233201953\n",
      "train loss:0.022606890240462457\n",
      "train loss:0.016570292855355426\n",
      "train loss:0.0150075352536986\n",
      "train loss:0.03199718891205511\n",
      "train loss:0.027023246033304624\n",
      "train loss:0.0180825775259155\n",
      "train loss:0.02060824364409944\n",
      "train loss:0.0151477593532902\n",
      "train loss:0.012418666143460474\n",
      "train loss:0.018342139730763995\n",
      "train loss:0.023908911552897233\n",
      "train loss:0.022921412235997463\n",
      "train loss:0.017297796220254517\n",
      "train loss:0.03057295123898559\n",
      "train loss:0.017848448619494563\n",
      "train loss:0.052197944559343394\n",
      "train loss:0.02353982811859763\n",
      "train loss:0.03539349537270634\n",
      "train loss:0.026185179526932948\n",
      "train loss:0.01959955978882548\n",
      "train loss:0.02483228831904305\n",
      "train loss:0.02872361919001853\n",
      "train loss:0.008768404093947682\n",
      "train loss:0.01728363642471041\n",
      "train loss:0.014892471373281218\n",
      "train loss:0.017974615892737852\n",
      "train loss:0.013065150874106157\n",
      "train loss:0.038438222514290826\n",
      "train loss:0.029083001860671042\n",
      "train loss:0.022283936029981005\n",
      "train loss:0.011269701949970456\n",
      "train loss:0.037309363752752685\n",
      "train loss:0.010770399682063121\n",
      "train loss:0.016365727484679456\n",
      "=== epoch:10, train acc:0.984, test acc:0.96 ===\n",
      "train loss:0.0314905357951779\n",
      "train loss:0.018214394536599118\n",
      "train loss:0.013092152733105322\n",
      "train loss:0.024569343480153445\n",
      "train loss:0.01788866817764413\n",
      "train loss:0.026804921550058546\n",
      "train loss:0.03501284570070102\n",
      "train loss:0.010455711456132423\n",
      "train loss:0.02717709440331266\n",
      "train loss:0.0181635581788336\n",
      "train loss:0.020502333084603534\n",
      "train loss:0.02777095785361996\n",
      "train loss:0.01621507377951408\n",
      "train loss:0.015494971005868273\n",
      "train loss:0.032030721250100716\n",
      "train loss:0.024228743920875838\n",
      "train loss:0.04912896657624581\n",
      "train loss:0.017010885141175425\n",
      "train loss:0.04504552930388263\n",
      "train loss:0.02857133274928149\n",
      "train loss:0.03283896797604839\n",
      "train loss:0.020828357714950712\n",
      "train loss:0.017716861213270225\n",
      "train loss:0.033064729726174934\n",
      "train loss:0.027576513358361886\n",
      "train loss:0.011263523143803045\n",
      "train loss:0.016616717664111057\n",
      "train loss:0.047616979763628144\n",
      "train loss:0.012771856589963541\n",
      "train loss:0.013683822909828402\n",
      "train loss:0.006000683943037458\n",
      "train loss:0.01518207708067207\n",
      "train loss:0.013713084277012042\n",
      "train loss:0.023076056767894752\n",
      "train loss:0.010920200627913686\n",
      "train loss:0.010607837883195073\n",
      "train loss:0.014538784615136167\n",
      "train loss:0.015973534913336078\n",
      "train loss:0.01979848126165538\n",
      "train loss:0.02393376919819742\n",
      "train loss:0.008886752598192207\n",
      "train loss:0.012512436331718002\n",
      "train loss:0.01335988808690394\n",
      "train loss:0.01766264344777044\n",
      "train loss:0.017958360487047215\n",
      "train loss:0.007926392749077362\n",
      "train loss:0.007343823262554443\n",
      "train loss:0.010153262638798868\n",
      "train loss:0.012748031373919824\n",
      "train loss:0.015214119713411564\n",
      "=== epoch:11, train acc:0.983, test acc:0.957 ===\n",
      "train loss:0.09185595420349375\n",
      "train loss:0.018680780341165718\n",
      "train loss:0.03462274231970415\n",
      "train loss:0.015565217504604935\n",
      "train loss:0.016297097443172363\n",
      "train loss:0.01204659533719199\n",
      "train loss:0.029119566615949295\n",
      "train loss:0.019704327882982882\n",
      "train loss:0.020024668551230834\n",
      "train loss:0.017197952235414572\n",
      "train loss:0.034872761708984276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.011837486475198616\n",
      "train loss:0.023686894848259775\n",
      "train loss:0.012886025435861809\n",
      "train loss:0.04252879131957203\n",
      "train loss:0.014516448274322967\n",
      "train loss:0.03051819319393665\n",
      "train loss:0.02067565956153437\n",
      "train loss:0.021086924193312644\n",
      "train loss:0.007794613508409868\n",
      "train loss:0.00884642859391071\n",
      "train loss:0.016280165579609762\n",
      "train loss:0.033352401264168025\n",
      "train loss:0.011209563214736378\n",
      "train loss:0.015200394084544178\n",
      "train loss:0.010905924822409582\n",
      "train loss:0.05397847140736874\n",
      "train loss:0.007037984384409269\n",
      "train loss:0.014058666495551446\n",
      "train loss:0.014350222257375771\n",
      "train loss:0.031596283761895466\n",
      "train loss:0.010653199863731803\n",
      "train loss:0.0436387200955656\n",
      "train loss:0.02026339842064205\n",
      "train loss:0.02483489891456807\n",
      "train loss:0.028767204845875315\n",
      "train loss:0.012922052308064709\n",
      "train loss:0.01838565587583223\n",
      "train loss:0.010300487008143993\n",
      "train loss:0.014378097325880333\n",
      "train loss:0.024288921101398294\n",
      "train loss:0.008656070938873151\n",
      "train loss:0.017219420825687396\n",
      "train loss:0.008759511275527774\n",
      "train loss:0.011187980970468376\n",
      "train loss:0.010883164880957205\n",
      "train loss:0.026786027423479603\n",
      "train loss:0.010409993977892948\n",
      "train loss:0.01661536895432757\n",
      "train loss:0.033838120759693086\n",
      "=== epoch:12, train acc:0.989, test acc:0.959 ===\n",
      "train loss:0.02061587000787011\n",
      "train loss:0.006860601959746332\n",
      "train loss:0.032432853523526706\n",
      "train loss:0.012791548874139224\n",
      "train loss:0.0353601557979622\n",
      "train loss:0.021215243075153455\n",
      "train loss:0.010275202816689174\n",
      "train loss:0.024048389443393273\n",
      "train loss:0.029118433313079374\n",
      "train loss:0.022871084322946603\n",
      "train loss:0.019948193471967954\n",
      "train loss:0.016537020323495806\n",
      "train loss:0.013446347848356261\n",
      "train loss:0.014147689179859207\n",
      "train loss:0.016408734189116383\n",
      "train loss:0.00948895306303562\n",
      "train loss:0.012072955060411544\n",
      "train loss:0.008884732617221091\n",
      "train loss:0.026190647510485564\n",
      "train loss:0.005223823254257029\n",
      "train loss:0.01772999729680399\n",
      "train loss:0.008935447371259366\n",
      "train loss:0.012064678525024497\n",
      "train loss:0.011627531659036232\n",
      "train loss:0.00684934342745986\n",
      "train loss:0.006302932243884934\n",
      "train loss:0.007705263236408047\n",
      "train loss:0.007713226732638112\n",
      "train loss:0.008452926242372642\n",
      "train loss:0.007731097436779322\n",
      "train loss:0.011615876882129941\n",
      "train loss:0.006474184012579983\n",
      "train loss:0.01896290210625698\n",
      "train loss:0.02796120008617976\n",
      "train loss:0.012653604242146366\n",
      "train loss:0.03886458385897006\n",
      "train loss:0.008614449241002124\n",
      "train loss:0.009604077396986922\n",
      "train loss:0.019037202992927955\n",
      "train loss:0.00749845768857863\n",
      "train loss:0.008027327533555244\n",
      "train loss:0.013723044222855313\n",
      "train loss:0.00832289459835983\n",
      "train loss:0.010293866314639839\n",
      "train loss:0.02863884636860912\n",
      "train loss:0.013951663483822515\n",
      "train loss:0.01318758547519042\n",
      "train loss:0.010690270349121296\n",
      "train loss:0.014741058943706668\n",
      "train loss:0.014415912202930023\n",
      "=== epoch:13, train acc:0.993, test acc:0.958 ===\n",
      "train loss:0.011266268776663455\n",
      "train loss:0.005828669555932321\n",
      "train loss:0.018663550273872787\n",
      "train loss:0.008274617760458567\n",
      "train loss:0.0048139358120600035\n",
      "train loss:0.008024124768630521\n",
      "train loss:0.008384145777122065\n",
      "train loss:0.016283621311733432\n",
      "train loss:0.014406494604705975\n",
      "train loss:0.009628976053706011\n",
      "train loss:0.009155907173798302\n",
      "train loss:0.005416624876423055\n",
      "train loss:0.007352265920797279\n",
      "train loss:0.01506511029754931\n",
      "train loss:0.009040802419586\n",
      "train loss:0.03761869857532633\n",
      "train loss:0.0133753454918934\n",
      "train loss:0.007198701285533615\n",
      "train loss:0.03740517099691367\n",
      "train loss:0.0077822290865078945\n",
      "train loss:0.006417521113467081\n",
      "train loss:0.010715309663925188\n",
      "train loss:0.013238407153596556\n",
      "train loss:0.0074156522035241194\n",
      "train loss:0.015811066751350702\n",
      "train loss:0.006318217083524664\n",
      "train loss:0.007981878623094926\n",
      "train loss:0.006379128378051319\n",
      "train loss:0.007694859881345463\n",
      "train loss:0.007698836479666131\n",
      "train loss:0.00600900549079733\n",
      "train loss:0.023752411567294095\n",
      "train loss:0.03112391171042221\n",
      "train loss:0.005602539127577847\n",
      "train loss:0.01187268065589226\n",
      "train loss:0.0128362194417054\n",
      "train loss:0.017695564237084915\n",
      "train loss:0.007343023679992069\n",
      "train loss:0.003785096168574241\n",
      "train loss:0.01149920577306548\n",
      "train loss:0.011506441414399195\n",
      "train loss:0.008110413003501763\n",
      "train loss:0.006225098223629051\n",
      "train loss:0.006001938281251423\n",
      "train loss:0.02859896826115243\n",
      "train loss:0.008426915261927103\n",
      "train loss:0.011233378510482618\n",
      "train loss:0.004587437255372854\n",
      "train loss:0.005789414730330076\n",
      "train loss:0.01334495922014041\n",
      "=== epoch:14, train acc:0.997, test acc:0.972 ===\n",
      "train loss:0.003810370556775467\n",
      "train loss:0.005401718231988155\n",
      "train loss:0.005088757754085998\n",
      "train loss:0.007730247730663304\n",
      "train loss:0.013036992539926524\n",
      "train loss:0.008813834057676962\n",
      "train loss:0.011574545744231867\n",
      "train loss:0.00662243201228432\n",
      "train loss:0.004870777633511654\n",
      "train loss:0.007787752127404719\n",
      "train loss:0.007070357218917812\n",
      "train loss:0.009010412814175452\n",
      "train loss:0.007801474450577167\n",
      "train loss:0.005665468074531405\n",
      "train loss:0.006873124637821894\n",
      "train loss:0.00526902769341627\n",
      "train loss:0.004389437081889745\n",
      "train loss:0.008321860701404225\n",
      "train loss:0.03542009962218325\n",
      "train loss:0.013217277627639544\n",
      "train loss:0.0067795805428066845\n",
      "train loss:0.0047751735541775794\n",
      "train loss:0.007324663397717884\n",
      "train loss:0.00893876002620662\n",
      "train loss:0.01032665060096583\n",
      "train loss:0.005245520844955657\n",
      "train loss:0.005312278874126339\n",
      "train loss:0.0032096386664548946\n",
      "train loss:0.003984317018250944\n",
      "train loss:0.004057762046112784\n",
      "train loss:0.0060296305025618405\n",
      "train loss:0.0070471657184678895\n",
      "train loss:0.0308630817401226\n",
      "train loss:0.007229889880458539\n",
      "train loss:0.00866405216059092\n",
      "train loss:0.007814414600968454\n",
      "train loss:0.0069477575018975805\n",
      "train loss:0.00734913780145648\n",
      "train loss:0.01229850133294738\n",
      "train loss:0.007719475657862228\n",
      "train loss:0.004455759488770842\n",
      "train loss:0.015507049470601983\n",
      "train loss:0.012355073708170537\n",
      "train loss:0.0038898646164011743\n",
      "train loss:0.006748999602060166\n",
      "train loss:0.004451747978441184\n",
      "train loss:0.004707637261185136\n",
      "train loss:0.009450309739132455\n",
      "train loss:0.0054570209815108\n",
      "train loss:0.0031192719244857395\n",
      "=== epoch:15, train acc:0.998, test acc:0.964 ===\n",
      "train loss:0.005018973317144914\n",
      "train loss:0.004781845280142428\n",
      "train loss:0.004280102521355069\n",
      "train loss:0.003811680175667775\n",
      "train loss:0.009406850831435767\n",
      "train loss:0.005469872515251384\n",
      "train loss:0.02410902472170069\n",
      "train loss:0.008483809884350348\n",
      "train loss:0.010550215720791298\n",
      "train loss:0.0034661967421963087\n",
      "train loss:0.003974220797780534\n",
      "train loss:0.004826334780182598\n",
      "train loss:0.012743488632105908\n",
      "train loss:0.008439806747877542\n",
      "train loss:0.004420689511109998\n",
      "train loss:0.0041847522454360395\n",
      "train loss:0.012608867933755343\n",
      "train loss:0.0033779363980919954\n",
      "train loss:0.004581285542490447\n",
      "train loss:0.0028597130736377034\n",
      "train loss:0.005462370669382642\n",
      "train loss:0.003966694810401519\n",
      "train loss:0.002380812110997228\n",
      "train loss:0.0056181974009916\n",
      "train loss:0.005382319955056983\n",
      "train loss:0.004344442760955078\n",
      "train loss:0.005714828259848681\n",
      "train loss:0.008381451267418665\n",
      "train loss:0.00999207225219969\n",
      "train loss:0.0038305530143982105\n",
      "train loss:0.004454507715341839\n",
      "train loss:0.004788946248905082\n",
      "train loss:0.006634995846029049\n",
      "train loss:0.003197979234512015\n",
      "train loss:0.0017738284613344377\n",
      "train loss:0.003353154356345406\n",
      "train loss:0.007188742300285148\n",
      "train loss:0.0242060019683611\n",
      "train loss:0.003991109178419004\n",
      "train loss:0.005716275827057095\n",
      "train loss:0.0022365355244750576\n",
      "train loss:0.006222393496316489\n",
      "train loss:0.0029464989715213873\n",
      "train loss:0.006315774617362874\n",
      "train loss:0.003405626408572302\n",
      "train loss:0.004177296357771389\n",
      "train loss:0.0041332564489171865\n",
      "train loss:0.0027295441479763937\n",
      "train loss:0.005420737013867881\n",
      "train loss:0.027013392862069215\n",
      "=== epoch:16, train acc:0.998, test acc:0.964 ===\n",
      "train loss:0.0031585351953721252\n",
      "train loss:0.0065704756950247314\n",
      "train loss:0.001645104124786423\n",
      "train loss:0.004567067017117555\n",
      "train loss:0.00405268056428406\n",
      "train loss:0.0032339355380561173\n",
      "train loss:0.0035828872168129104\n",
      "train loss:0.0066673642555074185\n",
      "train loss:0.01086637476286858\n",
      "train loss:0.008215062780809962\n",
      "train loss:0.008765520798336021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.005548205036036066\n",
      "train loss:0.002285591113278238\n",
      "train loss:0.002948322851471195\n",
      "train loss:0.004310403856940621\n",
      "train loss:0.0037781797867253797\n",
      "train loss:0.0028818119147124483\n",
      "train loss:0.014310795219576854\n",
      "train loss:0.028995378468887383\n",
      "train loss:0.0045570429187622155\n",
      "train loss:0.008950500479972397\n",
      "train loss:0.004289079999072621\n",
      "train loss:0.005202214428612482\n",
      "train loss:0.002396699672964216\n",
      "train loss:0.0034068359858082076\n",
      "train loss:0.0024305566677412767\n",
      "train loss:0.004889106354443868\n",
      "train loss:0.025590462805352977\n",
      "train loss:0.0018197927780436328\n",
      "train loss:0.00675764292024751\n",
      "train loss:0.007315409736903608\n",
      "train loss:0.0035875932936379495\n",
      "train loss:0.0035021079087300777\n",
      "train loss:0.002322553691284301\n",
      "train loss:0.0036237239585245346\n",
      "train loss:0.0025019871231622788\n",
      "train loss:0.0059072350938408225\n",
      "train loss:0.004045676368529753\n",
      "train loss:0.0053035416119591216\n",
      "train loss:0.0015517201766000648\n",
      "train loss:0.00379325774630024\n",
      "train loss:0.0035604858868656625\n",
      "train loss:0.0031077481797159127\n",
      "train loss:0.04063202565439632\n",
      "train loss:0.002009077800185556\n",
      "train loss:0.0050111440791273144\n",
      "train loss:0.0035150423965889873\n",
      "train loss:0.004800473861484091\n",
      "train loss:0.0027313793542876673\n",
      "train loss:0.003043215394340133\n",
      "=== epoch:17, train acc:0.998, test acc:0.966 ===\n",
      "train loss:0.0027353043145095057\n",
      "train loss:0.006700815839467348\n",
      "train loss:0.001765525691022188\n",
      "train loss:0.012179391317619925\n",
      "train loss:0.0033484234913361034\n",
      "train loss:0.0020763737382020054\n",
      "train loss:0.005364574145470379\n",
      "train loss:0.0025458371466062905\n",
      "train loss:0.00677174214602967\n",
      "train loss:0.0063598902443469454\n",
      "train loss:0.010408196083879606\n",
      "train loss:0.001649535785264355\n",
      "train loss:0.004116707162887128\n",
      "train loss:0.009447308436325097\n",
      "train loss:0.005271605425188697\n",
      "train loss:0.0019988893926643127\n",
      "train loss:0.0018986807316134604\n",
      "train loss:0.00560857638147203\n",
      "train loss:0.0024085780738207583\n",
      "train loss:0.0034573019261985856\n",
      "train loss:0.0024513164758807123\n",
      "train loss:0.004852250179664328\n",
      "train loss:0.006305674771226583\n",
      "train loss:0.001968791517390477\n",
      "train loss:0.0025445196046981057\n",
      "train loss:0.0028291426828871073\n",
      "train loss:0.003668011337177679\n",
      "train loss:0.004142152892585184\n",
      "train loss:0.0021640972397348524\n",
      "train loss:0.002277058259559001\n",
      "train loss:0.011149078999979169\n",
      "train loss:0.0028901668182570225\n",
      "train loss:0.003439058979325481\n",
      "train loss:0.002855941672113709\n",
      "train loss:0.0019928210639698933\n",
      "train loss:0.002729616304663715\n",
      "train loss:0.0016763636682993086\n",
      "train loss:0.004530090515660848\n",
      "train loss:0.0027868677673633757\n",
      "train loss:0.0021660805989172642\n",
      "train loss:0.004252799264122219\n",
      "train loss:0.0043193870575820685\n",
      "train loss:0.013611259226389817\n",
      "train loss:0.0022975856909543636\n",
      "train loss:0.013018809906864725\n",
      "train loss:0.003211450847315148\n",
      "train loss:0.009564435796371784\n",
      "train loss:0.0026159914661728926\n",
      "train loss:0.011273055886012214\n",
      "train loss:0.010329768163526896\n",
      "=== epoch:18, train acc:0.999, test acc:0.966 ===\n",
      "train loss:0.003699993227878114\n",
      "train loss:0.0014617049266144524\n",
      "train loss:0.0025352094284299986\n",
      "train loss:0.004131676785421436\n",
      "train loss:0.0016335879587222849\n",
      "train loss:0.0018772662228628773\n",
      "train loss:0.005619066790411326\n",
      "train loss:0.07303150841295333\n",
      "train loss:0.0044605339304996895\n",
      "train loss:0.007024469147550504\n",
      "train loss:0.002983203368243435\n",
      "train loss:0.0022040742365843184\n",
      "train loss:0.0028442323561346235\n",
      "train loss:0.010677943093113205\n",
      "train loss:0.002986571423298259\n",
      "train loss:0.004708053305665617\n",
      "train loss:0.0015982866605489575\n",
      "train loss:0.002001568609156227\n",
      "train loss:0.0019419538512533367\n",
      "train loss:0.005212213817957546\n",
      "train loss:0.0017423071213727435\n",
      "train loss:0.002594296510334548\n",
      "train loss:0.0020787028319660924\n",
      "train loss:0.0028680652304639855\n",
      "train loss:0.0022196442614628438\n",
      "train loss:0.004390532354511675\n",
      "train loss:0.002462610880717301\n",
      "train loss:0.003355241836995504\n",
      "train loss:0.004727896569663503\n",
      "train loss:0.002554378632165612\n",
      "train loss:0.002572993943646276\n",
      "train loss:0.00537898972127332\n",
      "train loss:0.0028768214910148263\n",
      "train loss:0.0045476563234704585\n",
      "train loss:0.0035747362521584996\n",
      "train loss:0.00548976644150143\n",
      "train loss:0.005500133292111931\n",
      "train loss:0.0023457010692017515\n",
      "train loss:0.0029260290064523696\n",
      "train loss:0.0044867365729829205\n",
      "train loss:0.0023901510288348387\n",
      "train loss:0.004348775099803039\n",
      "train loss:0.002261941857698981\n",
      "train loss:0.0024045932176241973\n",
      "train loss:0.008859563392043657\n",
      "train loss:0.0022604957019091987\n",
      "train loss:0.0018304669623921473\n",
      "train loss:0.0031364326631138128\n",
      "train loss:0.0020003091202947985\n",
      "train loss:0.002203065055769568\n",
      "=== epoch:19, train acc:0.999, test acc:0.966 ===\n",
      "train loss:0.0021028583234790595\n",
      "train loss:0.0031443782050885783\n",
      "train loss:0.0023052014616622867\n",
      "train loss:0.004904076844922772\n",
      "train loss:0.013695699112813604\n",
      "train loss:0.0024187467633411132\n",
      "train loss:0.017010098875598977\n",
      "train loss:0.011457120810265977\n",
      "train loss:0.036869289054852816\n",
      "train loss:0.001838070837482743\n",
      "train loss:0.00313193402380052\n",
      "train loss:0.0038913451889555717\n",
      "train loss:0.002224793297528571\n",
      "train loss:0.027954942560257795\n",
      "train loss:0.0014710972253947142\n",
      "train loss:0.001699854405395526\n",
      "train loss:0.011384995605710988\n",
      "train loss:0.0018105512401314849\n",
      "train loss:0.005276809397740051\n",
      "train loss:0.005011919962094877\n",
      "train loss:0.0025513868662511126\n",
      "train loss:0.0017519709963724841\n",
      "train loss:0.004091740628431616\n",
      "train loss:0.0031176017983816765\n",
      "train loss:0.004907493486683862\n",
      "train loss:0.0024648307689955026\n",
      "train loss:0.0020073482903102905\n",
      "train loss:0.0026924618190477295\n",
      "train loss:0.0016006794199797513\n",
      "train loss:0.00239121858472746\n",
      "train loss:0.004738131183246116\n",
      "train loss:0.0030901535214167154\n",
      "train loss:0.004324864674592125\n",
      "train loss:0.0013005550195150902\n",
      "train loss:0.0033549535268401435\n",
      "train loss:0.0012461448205884238\n",
      "train loss:0.00213691421362318\n",
      "train loss:0.0018284874323581184\n",
      "train loss:0.0019137781654975311\n",
      "train loss:0.002265952226128256\n",
      "train loss:0.003947593066883902\n",
      "train loss:0.0015825897942773612\n",
      "train loss:0.001388650636620755\n",
      "train loss:0.004841528244808543\n",
      "train loss:0.001856558252778124\n",
      "train loss:0.0018089654947681067\n",
      "train loss:0.0019002853387320153\n",
      "train loss:0.0029105962552122095\n",
      "train loss:0.0021573832314132507\n",
      "train loss:0.0019554784113194105\n",
      "=== epoch:20, train acc:1.0, test acc:0.973 ===\n",
      "train loss:0.0016627894554801553\n",
      "train loss:0.001744669565782461\n",
      "train loss:0.003256696738346807\n",
      "train loss:0.0010054589922422676\n",
      "train loss:0.0022624973411390563\n",
      "train loss:0.0013551628511291659\n",
      "train loss:0.0014106247857000181\n",
      "train loss:0.001979831852336471\n",
      "train loss:0.005652541922920973\n",
      "train loss:0.0019014778468584854\n",
      "train loss:0.001653936761315981\n",
      "train loss:0.002204082331580319\n",
      "train loss:0.0013255470801724761\n",
      "train loss:0.002132078688832148\n",
      "train loss:0.0015654076261622436\n",
      "train loss:0.00286936256409801\n",
      "train loss:0.0016990649347424947\n",
      "train loss:0.003357799997933753\n",
      "train loss:0.007048885472128143\n",
      "train loss:0.001523626017449334\n",
      "train loss:0.0011338517403888125\n",
      "train loss:0.001436564317447039\n",
      "train loss:0.0037414921009051656\n",
      "train loss:0.0019879788273110006\n",
      "train loss:0.0025121111872886738\n",
      "train loss:0.005999218071067759\n",
      "train loss:0.0013192080166681808\n",
      "train loss:0.007338539813440967\n",
      "train loss:0.0014536552888466797\n",
      "train loss:0.0013616067964740757\n",
      "train loss:0.002071860278700168\n",
      "train loss:0.0012631349980087088\n",
      "train loss:0.0013287253441289738\n",
      "train loss:0.0009400448559739516\n",
      "train loss:0.0010128153228225172\n",
      "train loss:0.001321630780921451\n",
      "train loss:0.0010802770216980242\n",
      "train loss:0.001894256864789911\n",
      "train loss:0.005510252365698025\n",
      "train loss:0.0014796339893370263\n",
      "train loss:0.0010937049717272469\n",
      "train loss:0.0032078280614410504\n",
      "train loss:0.004405333362054389\n",
      "train loss:0.0012003511988668773\n",
      "train loss:0.002020813477878074\n",
      "train loss:0.005897851605838504\n",
      "train loss:0.0014945957647964614\n",
      "train loss:0.0014660421659410635\n",
      "train loss:0.001226665105978082\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.97\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Network Parameters!\n"
     ]
    }
   ],
   "source": [
    "# パラメータの保存\n",
    "network.save_params(opti + \".params.pkl\")\n",
    "print(\"Saved Network Parameters!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xd8VfX9+PHXO4uElQABBQIyjCAKgkQciAsVUBRXLW7Uiv4cta3i+Naqta3S6ler/VottaAoTlyoWHCAE5SwN4SdABJGwkrIHe/fH+ckXsJNuBknN7l5Px+P+8i5n7Pe5xLuO59xPkdUFWOMMaa64qIdgDHGmIbNEokxxpgasURijDGmRiyRGGOMqRFLJMYYY2rEEokxxpga8TSRiMh4EdkmIksqWC8i8pyI5IjIIhE5MWTdDSKy2n3dEFLeX0QWu/s8JyLi5TUYY4ypnNc1kpeBoZWsHwZkuq/RwAsAItIaeAQ4GRgAPCIirdx9XgBuCdmvsuMbY4zxmKeJRFW/BnZWsskIYKI6ZgNpItIeGAJ8pqo7VXUX8Bkw1F3XUlVnq3Mn5UTgEi+vwRhjTOUSonz+jsCmkPe5blll5blhyg8hIqNxajk0a9asf8+ePWsvamOMpwr2+9i6uxhfIEhifBxHtkwmrWlivT9/UJVAMORVjZlDcncVEQgeul+cCGlNEw8+vnuOcNuXymzXguTE6tUZ5s6du11V2x5uu2gnEs+o6jhgHEBWVpZmZ2dHOSJjTCTeyd7EHz5YQro/WFYWnxDHtWd158xjDvudVmNfrcrnhZlrDjq/xAuDerenU5tm7C7yUVjko2B/ifOzyFdW5gvUfMqpdpWsa90sidSUxINeaU0TDylzyp1t05snkRBfvUQiIhsi2S7aiSQP6BTyPsMtywPOKlc+0y3PCLO9MaaeCQSVjTv3l33hlr32O1++oe+dL2Rnu2Jf8JBjHfAH+fvnq/n756ujcCXgCyjvL9gMQIvkhLIv77SUJNqnppAa8mWeVvpl3jSR5k0SiKvieKAbX55D/p4Dh5R3TEvmuwcG18r11LZoJ5IpwJ0i8iZOx3qhqm4RkWnA4yEd7OcDD6rqThHZLSKnAD8A1wP/iErkxphDbNq5n69X5/Pt6u18l7Od3cX+sNs1S4onNSWRlu5f1F3Sm5Kakkpa0yTGfb22wuNPuPEkr0Ivc+OEOWHLBch5/ALi47wdKPr7C47lwfcWU+QLlJWlJMYzZkj9bZ73NJGIyBs4NYt0EcnFGYmVCKCqLwJTgQuAHGA/cKO7bqeI/Ako/Rd9TFVLO+1vxxkNlgJ86r6MMVGwu9jH9zk7+DYnn29Wb2fDjv0AdEhNZujxR3JSl9akN29SljBSUxJpmZxIUkLFTS2fLNpCXkHRIeUd01I4u0dlDT+1o2NaStjzd0hL8TyJAFzSz+n2fXLaSjYXFNEhLYUxQ3qUlddH0himkbc+EmOq5oP5eWG/yPyBIAs2FfDN6u18szqfhbmFBIJKs6R4TunWhkGZ6Qw6pi3d0ptR3Vu8PpifF/Yv8icu610nX6bRPn99IiJzVTXrsNtZIjHGhAr3RZoYL/Q4ogUbduxnzwE/cQK9M9I4IzOd049Op1/nVpXWMqoTQzT/Io/2+esLSyQhLJEYE7lTn/iCLYXFh5THxwlXZnViUGY6p3VvQ1rTpChEZ+pSpIkk2p3txpgoK/EHmb9xl9NclbM9bBIBCAaVJy7rXcfRmYbAEokxjYyqsiZ/H9+4o6tmrd3B/pIA8XFC305ptEhOYE+Y0VYd0lKiEK1pCCyRGNMI7NxXwrc52/l2tTO6qrTW0aVNUy47sSODMttyavc2tExOrLCzecyQHtEK39RzlkiMiVHLNu/mo0Wb+WZ1Pks370YVWiYnMPDodO46py2DMtPp1LrpIfs1xOGnJroskRgTQ4p9AT5etIVJP2xg/sYCEuKEEzu34nfnHsPpmen0yUiL6F6IS/p1tMRhImaJxJgYsCZ/L5Nmb+TdebkUFvno3rYZDw/vxWUndrTRVcZzlkiMaaBK/EGmL9vKpNkbmbV2B4nxwpDjjuTaU47i5K6tq31DoDFVZYnEmAZm0879vPHjRt7O3sT2vSVktErhvqE9+EX/TrRt0STa4ZlGyBKJMfVQ+Tur7znvGFqmJDLphw3MXJWPAOf0PIJrT+nMGZltiauDOaCMqYglEmNqWe6u/XyyaAsfL9rCyq17aJmSQMvyU4ynJJLaNOngqcfdSQ1nr9nO45+uKJtOPa+giHveWYgC7Vo04a5zMhl5Uie7r8PUG5ZIjKkFWwqLypLHgk0FAJzQKY1RA7uw74C/7OFH2/eWsCZ/H4VFPnYX+4h0hiLFeajRdw+cQ2I1H1JkjFcskRhTTdv2FPPp4q18vGgzc9bvAuC4Di25f2hPhvdpH/YejVDBoLKn2F/2QCfnqXs+7npjftjtd+0rsSRi6iVLJMaEUdHsrzv2HuDTJVv5ZNEWfli3g6BCjyNacM95x3Bhn/Z0a9s84nPExYnTnFXuWeBjP11R4fMwjKmPLJEYU075KULyCooYM3khL8zMISd/H4Gg0q1tM+48J5OL+rQn84gWtXr+MUN62BQlpkGxRGJMOX/974qDvsTBeWZ3Tv4+bj2jG8P7dODY9i08u0/DpigxDY0lEtOoFfsCLN+ym8V5hSzKLWRxbmGl06jfN7RunpttU5SYhsTrZ7YPBZ4F4oGXVHVsufVHAeOBtsBO4FpVzRWRs4FnQjbtCYxU1Q9E5GXgTKDQXTdKVRd4eR0mNpT4g6z6aY+TMPIKWJRbyMqte/AHnaFTbZol0Scjlc2FRTaNujFV4FkiEZF44HngPCAXmCMiU1R1WchmTwETVfUVETkHeAK4TlVnAH3d47QGcoDpIfuNUdXJXsVuYkPhfh/Tlm1lcW4hi3ILWL5lDyUB596MtKaJ9O6Yyq1ndqN3xzT6ZKTSPjUZEbFp1I2pIi9rJAOAHFVdCyAibwIjgNBE0gv4nbs8A/ggzHGuAD5V1f0exmpiTP6eA4wcN4s1+fto0SSB4zumcuPALvTOSKVPxzQ6tU6psI/D+iiMqRovE0lHYFPI+1zg5HLbLAQuw2n+uhRoISJtVHVHyDYjgafL7fcXEXkY+AJ4QFUP1GrkpkHbsfcAV/97NpsLipl40wBOPzq9ylOIWB+FMZGL9t1N9wJnish8nH6PPKCsPUFE2gO9gWkh+zyI02dyEtAauD/cgUVktIhki0h2fn6+R+Gb+mbXvhKueekHNu3az39GZXHGMTYPlWnEdm+uk9N4mUjygE4h7zPcsjKqullVL1PVfsDv3bKCkE2uBN5XVV/IPlvUcQCYgNOEdghVHaeqWaqa1bZt29q5IlOvFex3ksi67ft46fqTOK17erRDMiY6crPhjavhmeNhe47np/MykcwBMkWkq4gk4TRRTQndQETSRaQ0hgdxRnCFugp4o9w+7d2fAlwCLPEgdtPAFBb5uO4/P5KzbS/jrs/i9ExLIlGlCt/8LzzXD7LHQ+DQUXCmlqnCmhnwykXw0mDY8B2cMQaatfH81J71kaiqX0TuxGmWigfGq+pSEXkMyFbVKcBZwBMiosDXwB2l+4tIF5wazVflDj1JRNoCAiwAbvPqGkzDsKfYxw3jf2TF1t3867r+nHmM1UCjShU+fwS+exZadICPfwuzX4DBj0DPC8EeuFW7gkFYOdVJ3JvnQfMj4fw/Q/9R0KR2Z12oiGik0482YFlZWZqdnR3tMIwH9h7wc8P4H1m4qYAXru3Peb2OiHZIjVswCFPvhez/QNZNcMFTsPJT+PxR2LEaOp0M5/0JOpcfd2OqLOCDJe/Ct89A/gpo1QUG/gZOuAoSk2vlFCIyV1WzDred3dluGqz9JX5umjCHBZsK+L+r+lkSibaAH6bcCQvfgNN+Dec95tQ+jh0OxwyF+a/CzCdg/PnQc7hTQ2l7TLSjbnh8RTD/NfjuOSjcCO16weX/gV6XQHx0vtItkZgGqagkwE0vzyF7w06eu6ofw3q3j3ZIjZu/BN69GZZPgbN/77TNhzZhxSdA1o3Q50qY9U+n2WvlKXDidXDWg9DiyOjFXt6TmbBv26HlzdrBmNVRPH9bOPVOmPW8sz7jJLjgb5A5BOKiOwDXEolpcIp9AX41cQ4/rtvJM7/sy/A+HaIdUuPmK4K3roOcz2DI43DqHRVvm9QMzhzjJJWv/uY0gS1629nntF9Dcsu6i7si4b7EKyuvs/PnO31P3c6GQeOhy+n1pr/JEolpUIp9AUa/Opfv1+zgqStOYERfu2kwqg7sgddHOiOELnrW6eCNRLN056/pU26DL/4EXz8J2RPgzPuc5X1h7v2qqxpBZX74V3TPf8sM6HhidGMIwxKJaTAO+APcPmkeX6/K52+X9+Hy/hnRDqlx278TJl0BmxfA5S9B7yuqfozW3eAXE+C0O+GzR+DT+yretrZqBKqw9yfYuQ52rYNd639e3rmu8n0ri68u1MMkApZITANR4g9yx6T5fLliG49f2psrT+p0+J2Md/Zug1cvhe2r4JevOsN6a6Jjf7jhI8j53ElOFVn3TdWO6y8+NFHsWg/+kCdQShy0zIDWXZzrmPdKxce77zCJpjb8rav356hllkhMvecLBPn1G/P5fPlP/GnEcVx9cudoh9S4FebCxBHO9BtXvwXdz6md44pA5nmVb/PK8OodO7GpMzy2dTcn3tZdoVVX52dqJ0hI+nnbyhJJ09bVO3+Ms0Ri6qXQZ6YnJ8ZR5AvyyEW9uO7ULtEOrXHbuRZeGQHFBXDte3DUqXV7/hs+rtr28UnQ6ihofkTkHdPN2lU8aqsuRPv81WCJxNQ75Z8HUuQLkhgvtGqadJg9a0m0h3/WV9tWODWRQAncMAU69Kv7GLoO8v4c0f43jvb5qyHas/8ac4gnp60M+8z0J6etrJsAoj38sz7avAAmDAMUbpzqbRKp6C/vevwXeWNnNRJTr6gqeQVFYddtrqC8Tj1VxTuxk5pD+xOcL94O/Zzl+nCvRGUqqpFJHNyZDW26e3v+BvgXeWNnicTUGzv3lfCHDyqezLlePDO9x7Cqbb9/pzOl99L3fi5rk/lzYunQD9r3cW7UK1WdprUDe53RSKEjk3atc86fnAopaZDSCpLTKl9u0rLimpcGvU8ipkGyRGLqhelLt/I/7y+msMjH8D5H8vnybRT7gmXr6+yZ6UW7Kl9/0bPVO+6+7U7z0Ob5zmv9t7D4bWedxEHbnj8nlsqa1jbODn//Q/kb+JLTnBFJzdrCgd1O/0bRLqeTPFBScZwSX73rM42aJRITVYVFPv740VLem5dHr/Ytee1XJ9PzyJYHjdqqs2emF2yq/B6GmmiWDpnnOq9Se7aGJJd5sGoaLJhU+XHGD3EXBFIznCGtPYY5P0uHs7bq4tQywlEF334oKnCSStGuQ5e/earm12saFUskJmq+XpXPfZMXkb/3AL8enMmdZx9NUoIz/qPOn5m+ZRFM+oUzb1RymvPFWl5td/a2OBJ6DHVe4HzJ786DZ46reJ9rJjsJI60TJDSp+jlFnGa0pGaQWsHna4nEVJElElPn9h3w8/jU5Uz6YSNHt2vOuOv70ycjLXoB5XwBb1/v9CXc9F84old04hC3llGZw92wZ0wUWCIxdWr22h2MmbyQ3F1F3HpGN3573jEkJ0axXX7B6zDlLqeP4pp3oKXNJNwQb4gz0WWJxNSJYl+Av/13JRO+X0fn1k1559ZTyeoSxekmVOHrp2DGn6Hrmc58Ucmp0YsnVLS/yG34rakiSyTGc/M37uKedxayNn8f1596FA8M60nTpCj+6gX88MnvnDmV+oyEi/9x8FxL0WZf5KaB8fR/s4gMBZ4F4oGXVHVsufVHAeOBtsBO4FpVzXXXBYDF7qYbVfVit7wr8CbQBpgLXKeqlYxnNNFS7Avwjy9X88LMNRzZMplJvzqZgUenRzeoA3th8o2wejoMuhfOeajePBzImIbKs0QiIvHA88B5QC4wR0SmqOqykM2eAiaq6isicg7wBHCdu65IVfuGOfRfgWdU9U0ReRG4GXjBq+swkfEFgqz6aQ+LcwtZmFvI4rwCVm7dgy+gXJmVwUPDe9EyOTG6Qe7d5ozM2roIhj8DWTdFNx5jYoSXNZIBQI6qrgUQkTeBEUBoIukF/M5dngF8UNkBRUSAc4Cr3aJXgEexRFLrKruPIxBUcrbtZVFuAYvzClmUW8iyLbsp8Ts3ELZMTqBPRhq/GtSNM49pyynd2lTt5F5Mmrh9Nbx2uXPj3sg3fh5ya4ypMS8TSUdgU8j7XODkctssBC7Daf66FGghIm1UdQeQLCLZgB8Yq6of4DRnFaiqP+SYYQfDi8hoYDRA5872/IqqKD/7bl5BEWMmL+S9ebnsLwmwdPPusnXNmyRwfMeWjDqtC707ptInI5XOrZsiNWkuqu1JEzf+AG/80rlre9THzkOUjDG1Jtqd7fcC/ycio4CvgTygdNrXo1Q1T0S6AV+KyGKgMNIDq+o4YBxAVlaW1mrUMa6i2Xe/Wb2d/ke1YuSATvTJSKVPRhpd2zQjLq4W+xj0MP9Un97vzgvVypkbKnS5dM6o0I7zZVPgvVugZUe4drLzYCNjTK3yMpHkAaHPQ81wy8qo6macGgki0hy4XFUL3HV57s+1IjIT6Ae8C6SJSIJbKznkmKbmKptld/L/O632T7g3H9bOhLUzYM2Myrdd8Lozd1RlEpu6SSUVti2HjCy46i1oVsUmNmNMRLxMJHOATHeUVR4wkp/7NgAQkXRgp6oGgQdxRnAhIq2A/ap6wN1mIPA3VVURmQFcgTNy6wbgQw+voVEpvdejojpBrc2+6yuGjbPcxPElbHUH5yWnQbezYFklXWUPbnKG7xYXuvNDFfw8GWG4eaO6DIJzH4WkprUTuzHmEJ4lElX1i8idwDSc4b/jVXWpiDwGZKvqFOAs4AkRUZymrTvc3Y8F/iUiQZyHb40NGe11P/CmiPwZmA/8x6traExC7/U4/eg2ZG/YVXuz76rCT0t/Thwbvgd/McQlQqeT4Zw/QPezoX1fiIuHRw9zY2B8glO7sBqGMfWCp30kqjoVmFqu7OGQ5cnA5DD7fQ/0ruCYa3FGhJlacMAf4NnPV/PiV2ton5pSdq9Hrcy+u3am0xS1dibs/ckpS+8B/W90EsdRA6FJ80P3i/ad3caYKhE9XOdmDMjKytLs7Oxoh1HvLMkr5N53FrJi6x5+mdWJh4YfS4vauNdjyyL4/BGn9pHS2kka3c+BbmdXPOOsMabeEZG5qpp1uO2iPWrLRIEvEOSFmWt47ovVtGqWxPhRWZzT84iaH3jXBpjxF1j0tjOKasjjcNKvqjfduTGmwbBE0sis/mkP97yzkEW5hVx8Qgf+ePFxtGpWw3mm9u+Eb/4XfhznPO3v9N/AwN84ycQYE/MskTQSgaAy/tt1PDl9Jc2S4nn+6hO5sE/7mh3UVwQ/vAjfPAMle6Dv1XDW/1jzlTGNjCWSRmD99n3c+85Csjfs4rxeR/D4pb1p26IGzU3BACx8A2Y87jzR75ihMPiR6D0QyhgTVZZIYlgwqLz2wwaemLqChHjh6StP4NJ+Has/fYmq81zxzx+F/OXOVCOXjYMup9dq3MaYhsUSSQx7fOpyXvp2HYMy0/nbFX1onxrhDYUVTZoYlwhBnzPNyC9egV4jbAp2Y4wlkli1a18Jr87ewKX9OvL0lSdUrRZS0eSIQR9c8BT0HwXxUZ4S3hhTb1giiVFvzNnIAX+Q287sXrOZeMsbcEvtHcsYExMskcQgXyDIq7M2MPDoNvQ4skXkOx7YC4vf8S4wY0xMskQSg6Yt3cqWwmL+NOL4yHbYugSyxzs3Epbs8TY4Y0zMsUQSgyZ8t56j2jTlnJ6VzE3lK3Zm2c0eD5t+gPgmcNylcNLN8J/z6i5YY0yDZ4kkxizKLWDuhl08PLxX+AdO7VjjJI8Fk5yp1lt3h/P/4txM2LS1s41NmmiMqQJLJDFmwnfrad4kgV9kZfxcGPDBik+cBLLuK4hLgJ4XQtZN0PXMQ4fwVve56MaYRskSSQzZtruYjxdt5pqTj3Jm8d233ZnCZN5EZxr31E5wzkPQ7zpocWS0wzXGxAhLJDHktR824g8qo07r4jxF8LXLnCndM893ah+Z5zkPjjLGmFpkiSRGHPAHeP2HDZzTox1d0pvBd8/CloVwxQQ4/rJoh2eMiWFx0Q7A1I6PFm5h+94SRg3s4nSoz3gcelzojMQyxhgPeZpIRGSoiKwUkRwReSDM+qNE5AsRWSQiM0Ukwy3vKyKzRGSpu+6XIfu8LCLrRGSB++rr5TU0BKrKhO/WkdmuOad3bwMf/wbik+DCp2wuLGOM5zxLJCISDzwPDAN6AVeJSPl5xp8CJqpqH+Ax4Am3fD9wvaoeBwwF/i4ioU9JGqOqfd3XAq+uoaGYs34XSzfvZtTALsjC12Hd13Duo9CyQ7RDM8Y0Al7WSAYAOaq6VlVLgDeBEeW26QV86S7PKF2vqqtUdbW7vBnYBrT1MNYGbcJ360hNSeSyzESY9nvofCr0vzHaYRljGgkvE0lHYFPI+1y3LNRCoLQn+FKghYi0Cd1ARAYAScCakOK/uE1ez4hI2Cc0ichoEckWkez8/PyaXEe9lrtrP9OWbmXkgE6kfPE/4NsPFz0Hcdb9ZYypG9H+trkXOFNE5gNnAnlAoHSliLQHXgVuVNWgW/wg0BM4CWgN3B/uwKo6TlWzVDWrbdvYrcy8OmsDIsLodith6ftwxn3Q9phoh2WMaUS8HP6bB3QKeZ/hlpVxm60uAxCR5sDlqlrgvm8JfAL8XlVnh+yzxV08ICITcJJRo7S/xM8bP25kRM/mtJl5K7TrBQPvjnZYxphGxssayRwgU0S6ikgSMBKYErqBiKSLSGkMDwLj3fIk4H2cjvjJ5fZp7/4U4BJgiYfXUK+9Ny+P3cV+7k96C/ZsgYv/AQlJ0Q7LGNPIeJZIVNUP3AlMA5YDb6vqUhF5TEQudjc7C1gpIquAI4C/uOVXAmcAo8IM850kIouBxUA68GevrqE+U1Ve/n49v2iXyxErXoOTb4OMrGiHZYxphERVox2D57KysjQ7OzvaYdSqr1fl86vx35Gd/kdaxvvg9tnQpHm0wzLGxBARmauqh/0L1aZIaaAmfLeOe1M+puXetXDNZEsixpioiahpS0TeE5ELQ/ozTBStzd9L3qp53Mz70PtKZzJGY4yJkkgTwz+Bq4HVIjJWRHp4GJM5jInfreFvif9GmrSEoU8cfgdjjPFQRIlEVT9X1WuAE4H1wOci8r2I3CgiiV4GaA62u9hH4rzx9I3LIW7YWGiWHu2QjDGNXMRNVe4d56OAXwHzgWdxEstnnkRmwvrkmx/5jbzBnowzoc+V0Q7HGGMi62wXkfeBHjh3mV8UclPgWyISW8Oh6rFAIEiXWX8gXqDZ5f+wmX2NMfVCpKO2nlPVGeFWRDI0zNSOpdPHc2pwLkv7PMhxrY6KdjjGGANE3rTVK3QadxFpJSK3exSTCWffDrrMeYxlcjQ9Lron2tEYY0yZSBPJLaVzYAGo6i7gFm9CMuEUfngfKYG9LO7/ZxISbXyDMab+iDSRxLtzWwFlD62ySZ3qSs4XpK6azEt6MeefPTja0RhjzEEi7SP5L07H+r/c97e6ZcYLT2bCvm2HFF+bOJMWzSx/G2Pql0gTyf04yeP/ue8/A17yJCITNokAtAjsquNAjDHm8CJKJO5DpV5wX8YYY0yZSO8jyQSewHnGenJpuap28yguY4wxDUSkne0TcGojfuBsYCLwmldBGWOMaTgiTSQpqvoFzvNLNqjqo8CF3oVljDGmoYi0s/2AO4X8ahG5E+fZ6/YADI/sII02FFRQbowx9UukieRuoCnwa+BPOM1bN3gVVGM3qPhp5ja5jbcCZ/Gof1RZuQDrohaVMcaEd9imLffmw1+q6l5VzVXVG1X1clWdHcG+Q0VkpYjkiMgDYdYfJSJfiMgiEZkpIhkh624QkdXu64aQ8v4istg95nOhN0rGimEpy0iREqYFTzqovENaSpQiMsaYih02kahqADi9qgd2E9DzwDCc0V5XiUivcps9BUxU1T7AYzgjwxCR1sAjwMnAAOAREWnl7vMCzvQsme5raFVjq88W5xYyyD+LndqcH4M9y8pTEuMZM8SeJ2aMqX8i7WyfLyJTROQ6Ebms9HWYfQYAOaq6VlVLgDeBEeW26QV86S7PCFk/BPhMVXe683p9BgwVkfZAS1WdraqKM3rskgivod7btqeY2yfO4tz4+ezMOJcj05ojQMe0FJ64rDeX9OsY7RCNMeYQkfaRJAM7gHNCyhR4r5J9OgKbQt7n4tQwQi0ELsN5SNalQAv3AVrh9u3ovnLDlB9CREYDowE6d+5cSZj1wwF/gNtenUvPogU0j9/P0WdezXfHnHP4HY0xJsoivbP9Ro/Ofy/wfyIyCvgaZzRYoDYOrKrjgHEAWVlZWhvH9Iqq8tD7S5i3sYCve+bA5hbQ9cxoh2WMMRGJ9M72CTg1kIOo6k2V7JYHdAp5n+GWhe6/GadGgog0By5X1QIRyQPOKrfvTHf/jHLlBx2zIZrw3XremZvL3Wd3pfPCGXDM+ZCYfPgdjTGmHoi0j+Rj4BP39QXQEth7mH3mAJki0lVEkoCRwJTQDUQk3b0/BeBBYLy7PA04332AVivgfGCa+4jf3SJyijta63rgwwivoV76ZnU+f/5kGef3OoK7j9kJ+7fDsRdFOyxjjIlYpE1b74a+F5E3gG8Ps4/fvXlxGhAPjFfVpSLyGJCtqlNwah1PiIjiNG3d4e67U0T+hJOMAB5T1Z3u8u3Ay0AK8Kn7apDWbd/Hna/PJ7NdC575ZV/ivnwI4pvA0edFOzRjjIlYpJ3t5WUC7Q63kapOBaaWK3s4ZHkyMLmCfcfzcw0ltDwbOL6K8dY7e4p93DIxmziBl27IollSPCz/CI4eDE1s0gBjTMMRaR/JHg7uI9mK84wSUw2BoHL3mwtYv30fr958Mp1aN4W8ebA7F875fbTDM8aYKom0aauF14E0Jk9NX8mXK7bxpxHHcWp3d/as5R+BxMMxMXV9skAXAAAZS0lEQVR/pTGmEYios11ELhWR1JD3aSISMzcC1qUPF+Txwsw1XH1yZ6495SinUBWWT4Gug6Bp6+gGaIwxVRTpqK1HVLWw9I2qFuBMYWKqYFFuAfdNXsSALq159KLjKJsmLH8l7Mix0VrGmAYp0kQSbrvqdtQ3Stt2FzN64lzSmzfhhWtPJCkh5CNd/hEg0HN41OIzxpjqijSRZIvI0yLS3X09Dcz1MrBYcsAf4NbX5lJY5OPf12fRpnmTgzdYPgU6DYAWR0YnQGOMqYFIE8ldQAnwFs7ki8W493yYyqkqv39/CfM3FvD0lSfQq0PLgzfYtR62LrJmLWNMgxXpqK19wCHPEzGHN/679Uyem8vdgzMZ1rv9oRss/9j5ac1axpgGKtJRW5+JSFrI+1YiMs27sGLDt6u385dPljHkuCO4e3Bm+I2WfwRH9obWXes2OGOMqSWRNm2luyO1AHCfEXLYO9sbu4mz1tO2RROevrIvcXFhHuS4Zyts+gGOvbjOYzPGmNoSaSIJikjZQz1EpAthZgM2Byv2B2mfmkKzJhW0IK74BFDrHzHGNGiRDuH9PfCtiHwFCDAI96FRpmL+QJCk+Epy9fKPoM3R0LZnxdsYY0w9F1GNRFX/C2QBK4E3gHuAIg/jigm+QJCE+DBNWgD7d8L6b5zaiFSwjTHGNACRTtr4K+BunAdJLQBOAWZx8KN3TTklAaVpUgW5etU0CPqtWcsY0+BF2kdyN3ASsEFVzwb6AQWV72J8/iCJFTVtLf8IWnaEDifWbVDGGFPLIk0kxapaDCAiTVR1BdDDu7Bigy8QJCkhTLPVgb2w5gtr1jLGxIRIO9tz3ftIPgA+E5FdwAbvwooNvkAFNZKcz8FfbM1axpiYEOmd7Ze6i4+KyAwgFfivZ1HFCF9ASYgLk0iWfwRN06HzqXUflDHG1LJIm7bKqOpXqjpFVUsOt62IDBWRlSKSIyKHTLEiIp1FZIaIzBeRRSJygVt+jYgsCHkFRaSvu26me8zSdfX2xsiwTVv+A05He88LIC4+OoEZY0wt8mwqeBGJB54HzgNygTkiMkVVl4Vs9hDwtqq+ICK9cJ7v3kVVJwGT3OP0Bj5Q1QUh+13jPru9XgvbtLX2KyjZY3ezG2NiRpVrJFUwAMhR1bVu7eVNYES5bRQonQ43Fdgc5jhXufs2OL6AHppIlk+BJi2h6xnRCcoYY2qZl4mkI7Ap5H2uWxbqUeBaEcnFqY3cFeY4v8S5CTLUBLdZ6w8i4Yc9ichoEckWkez8/PxqXUBNlZSvkQT8sHIqHDMEEppUvKMxxjQgXiaSSFwFvKyqGcAFwKsiUhaTiJwM7FfVJSH7XKOqvXGmaRkEXBfuwKo6TlWzVDWrbdu23l1BJZwpUkLy3MZZsH+HjdYyxsQULxNJHtAp5H2GWxbqZuBtAFWdBSQD6SHrR1KuNqKqee7PPcDrOE1o9U4gqAQVEuLLPVI3IRmOPjd6gRljTC3zMpHMATJFpKuIJOEkhSnlttkIDAYQkWNxEkm++z4OuJKQ/hERSRCRdHc5ERgOLKEe8gWCAD83bQWDTiI5+lxIahbFyIwxpnZ5lkhU1Q/cCUwDluOMzloqIo+JSOmQpXuAW0RkIU7NY5Sqlk5PfwawSVXXhhy2CTBNRBbhzPmVB/zbq2uoiZKyROI2bW2eD3s2W7OWMSbmeDb8F0BVp+J0ooeWPRyyvAwYWMG+M3Emhwwt2wf0r/VAPeDzO4kkKcHN1cunQFyC09FujDExJNqd7THLF3AqVonxcaDqJJKuZ0BKqyhHZowxtcsSiUdK+0gS4gS2LYeda61ZyxgTkyyReKQ0kSQlxDmd7Aj0uDC6QRljjAcskXjkoKat5R9B51OgxRFRjsoYY2qfJRKPlNZIWhblwk+LrVnLGBOzLJF4pHT4b/vNnzkFPYdHMRpjjPGOJRKP+N2mrba506H9CdDqqChHZIwx3rBE4hFfIEg7dtFy+3xr1jLGxDRLJB4pCQQ5P959ZIo9e8QYE8MskXjE5w9yWtxSSlp0grY9oh2OMcZ4xhKJR3wBpSkHCKS0iXYoxhjjKUskHvEHgyTiR+KToh2KMcZ4yhKJR0r8QRLFD5ZIjDExzhKJR3wBJQk/kmCJxBgT2yyReMQXcJu2LJEYY2KcJRKPlCUSa9oyxsQ4SyQeKXETSZzVSIwxMc4SiUf8ASVRAta0ZYyJeZ4mEhEZKiIrRSRHRB4Is76ziMwQkfkiskhELnDLu4hIkYgscF8vhuzTX0QWu8d8TkTEy2uoLl8gSJLVSIwxjYBniURE4oHngWFAL+AqEelVbrOHgLdVtR8wEvhnyLo1qtrXfd0WUv4CcAuQ6b6GenUNNVHatGXDf40xsc7LGskAIEdV16pqCfAmMKLcNgq0dJdTgc2VHVBE2gMtVXW2qiowEbikdsOuHT6/M/zXEokxJtZ5mUg6AptC3ue6ZaEeBa4VkVxgKnBXyLqubpPXVyIyKOSYuYc5JgAiMlpEskUkOz8/vwaXUT2+QOkNiYl1fm5jjKlL0e5svwp4WVUzgAuAV0UkDtgCdHabvH4HvC4iLSs5ziFUdZyqZqlqVtu2bWs98MPxBwJWIzHGNAoJHh47D+gU8j7DLQt1M24fh6rOEpFkIF1VtwEH3PK5IrIGOMbdP+Mwx6wX/H6/s2A1EmNMjPOyRjIHyBSRriKShNOZPqXcNhuBwQAiciyQDOSLSFu3sx4R6YbTqb5WVbcAu0XkFHe01vXAhx5eQ7Wp/4CzYDUSY0yM86xGoqp+EbkTmAbEA+NVdamIPAZkq+oU4B7g3yLyW5yO91GqqiJyBvCYiPiAIHCbqu50D3078DKQAnzqvuod9Zc4C5ZIjDExzsumLVR1Kk4nemjZwyHLy4CBYfZ7F3i3gmNmA8fXbqS1TwOWSIwxjUO0O9tjVtBqJMaYRsISiUckYH0kxpjGwRKJV8qatmzUljEmtlki8YgGfM6C1UiMMTHOEolXrI/EGNNIWCLxSlmNxJq2jDGxzRKJR+KC1rRljGkcLJF4xe4jMcY0EpZIPCJBa9oyxjQOlkg8Ehe0GokxpnGwROIRCZbO/muJxBgT2zyda6sxk2CJM1WlNW0Z0yD5fD5yc3MpLi6OdiieS05OJiMjg8TE6n1fWSLxSHzQ5yYSq5EY0xDl5ubSokULunTpgvPUitikquzYsYPc3Fy6du1arWNY05YHAkElAWvaMqYhKy4upk2bNjGdRABEhDZt2tSo5mWJxAO+QJBE7AmJxjR0sZ5EStX0Oi2ReKAkECSJgPPGaiTGmBhnicQDPr/VSIxpbD6Yn8fAsV/S9YFPGDj2Sz6Yn1ej4xUUFPDPf/6zyvtdcMEFFBQU1OjcVWWJxAP+oJIofoISD3Hx0Q7HGOOxD+bn8eB7i8krKEKBvIIiHnxvcY2SSUWJxO/3V7rf1KlTSUtLq/Z5q8PTUVsiMhR4Fmf80kuqOrbc+s7AK0Cau80DqjpVRM4DxgJJQAkwRlW/dPeZCbQHitzDnK+q27y8jqoqcWskQUm0TG1MDPjjR0tZtnl3hevnbyygJBA8qKzIF+C+yYt448eNYffp1aElj1x0XIXHfOCBB1izZg19+/YlMTGR5ORkWrVqxYoVK1i1ahWXXHIJmzZtori4mLvvvpvRo0cD0KVLF7Kzs9m7dy/Dhg3j9NNP5/vvv6djx458+OGHpKSkVOMTqJxn33MiEg88DwwDegFXiUivcps9BLytqv2AkUBp+t0OXKSqvYEbgFfL7XeNqvZ1X/UqiYDT2Z6EH42zZi1jGoPySeRw5ZEYO3Ys3bt3Z8GCBTz55JPMmzePZ599llWrVgEwfvx45s6dS3Z2Ns899xw7duw45BirV6/mjjvuYOnSpaSlpfHuu+9WO57KeFkjGQDkqOpaABF5ExgBLAvZRoGW7nIqsBlAVeeHbLMUSBGRJqp6wMN4a40voE6NxPpHjIkJldUcAAaO/ZK8gqJDyjumpfDWrafWSgwDBgw46D6P5557jvfffx+ATZs2sXr1atq0aXPQPl27dqVv374A9O/fn/Xr19dKLOV52fLSEdgU8j7XLQv1KHCtiOQCU4G7whzncmBeuSQyQUQWiMgfpB6Ozysd/ms1EmMahzFDepCSeHB/aEpiPGOG9Ki1czRr1qxseebMmXz++efMmjWLhQsX0q9fv7D3gTRp0qRsOT4+/rD9K9UV7Sb8q4CXVTUDuAB4VUTKYhKR44C/AreG7HON2+Q1yH1dF+7AIjJaRLJFJDs/P9+zCwjHFwiSKJZIjGksLunXkScu603HtBQEpybyxGW9uaRf+b+dI9eiRQv27NkTdl1hYSGtWrWiadOmrFixgtmzZ1f7PLXBy6atPKBTyPsMtyzUzcBQAFWdJSLJQDqwTUQygPeB61V1TekOqprn/twjIq/jNKFNLH9yVR0HjAPIysrS2rqoSPgC6vSR2D0kxjQal/TrWKPEUV6bNm0YOHAgxx9/PCkpKRxxxBFl64YOHcqLL77IscceS48ePTjllFNq7bzV4WUimQNkikhXnAQyEri63DYbgcHAyyJyLJAM5ItIGvAJziiu70o3FpEEIE1Vt4tIIjAc+NzDa6gWp2krYDcjGmNq5PXXXw9b3qRJEz799NOw60r7QdLT01myZElZ+b333lvr8ZXyrGlLVf3AncA0YDnO6KylIvKYiFzsbnYPcIuILATeAEapqrr7HQ087PaFLBCRdkATYJqILAIW4CSof3t1DdXl3NnusxqJMaZR8PQ+ElWditOJHlr2cMjyMmBgmP3+DPy5gsP2r80YveDzB0nBD/FNDr+xMcY0cNHubI9JvoCSJH7EaiTGmEbAEokH/EF3ri1LJMaYRsASiQdKp0ixGokxpjGwROIB5872AJJgicQYE/vsUbseKL2z3RKJMY3Ek5mwL8y0f83awZjV1TpkQUEBr7/+OrfffnuV9/373//O6NGjadq0abXOXVVWI/FA6aSNcZZIjGkcwiWRysojUN3nkYCTSPbv31/tc1eV1Ug84As4zyOxGokxMeLTB2Dr4urtO+HC8OVH9oZhY8Ov4+Bp5M877zzatWvH22+/zYEDB7j00kv54x//yL59+7jyyivJzc0lEAjwhz/8gZ9++onNmzdz9tlnk56ezowZM6oXdxVYIvFAadNWXILdR2KMqZ6xY8eyZMkSFixYwPTp05k8eTI//vgjqsrFF1/M119/TX5+Ph06dOCTTz4BnDm4UlNTefrpp5kxYwbp6el1EqslEg9Y05YxMaaSmgMAj6ZWvO7GT2p8+unTpzN9+nT69esHwN69e1m9ejWDBg3innvu4f7772f48OEMGjSoxueqDkskHigJlN5HYrP/GmNqTlV58MEHufXWWw9ZN2/ePKZOncpDDz3E4MGDefjhh8McwVvW2e4Bn1/thkRjGpNm7apWHoHQaeSHDBnC+PHj2bt3LwB5eXls27aNzZs307RpU6699lrGjBnDvHnzDtm3LliNxAN+v494UUskxjQW1RziW5nQaeSHDRvG1VdfzamnOk9bbN68Oa+99ho5OTmMGTOGuLg4EhMTeeGFFwAYPXo0Q4cOpUOHDtbZ3lBpoMRZsKYtY0wNlJ9G/u677z7offfu3RkyZMgh+911113cdVe4B856w5q2PBD0lSYSq5EYY2KfJRIPqN8SiTGm8bBE4oFg4ICzYE1bxjRoznP2Yl9Nr9MSiResRmJMg5ecnMyOHTtiPpmoKjt27CA5Obnax7DOdg9owOcsWCIxpsHKyMggNzeX/Pz8aIfiueTkZDIyMqq9vyUSL9ioLWMavMTERLp27RrtMBoET5u2RGSoiKwUkRwReSDM+s4iMkNE5ovIIhG5IGTdg+5+K0VkSKTHrBcC1rRljGk8PEskIhIPPA8MA3oBV4lIr3KbPQS8rar9gJHAP919e7nvjwOGAv8UkfgIjxl1PzdtWY3EGBP7vKyRDAByVHWtqpYAbwIjym2jQEt3ORXY7C6PAN5U1QOqug7IcY8XyTGjzzrbjTGNiJd9JB2BTSHvc4GTy23zKDBdRO4CmgHnhuw7u9y+Hd3lwx0TABEZDYx23+4VkZVVjL9UOrC9qjs9CfDHM6t5yiqpVnx1yOKrGYuvZiy+mjkqko2i3dl+FfCyqv6viJwKvCoix9fGgVV1HDCupscRkWxVzaqFkDxh8dWMxVczFl/N1Pf4IuVlIskDOoW8z3DLQt2M0weCqs4SkWScDF3Zvoc7pjHGmDrkZR/JHCBTRLqKSBJO5/mUcttsBAYDiMixQDKQ7243UkSaiEhXIBP4McJjGmOMqUOe1UhU1S8idwLTgHhgvKouFZHHgGxVnQLcA/xbRH6L0/E+Sp3bSJeKyNvAMsAP3KGqAYBwx/TqGlw1bh7zmMVXMxZfzVh8NVPf44uIxPrt/8YYY7xlc20ZY4ypEUskxhhjasQSiSuC6VyaiMhb7vofRKRLHcbWyZ1KZpmILBWRu8Nsc5aIFIrIAvf1cF3F555/vYgsds+dHWa9iMhz7ue3SEROrMPYeoR8LgtEZLeI/KbcNnX6+YnIeBHZJiJLQspai8hnIrLa/dmqgn1vcLdZLSI31GF8T4rICvff730RSatg30p/FzyM71ERyQv5N7yggn09n2apgvjeColtvYgsqGBfzz+/Wqeqjf6F03G/BugGJAELgV7ltrkdeNFdHgm8VYfxtQdOdJdbAKvCxHcW8HEUP8P1QHol6y8APgUEOAX4IYr/1luBo6L5+QFnACcCS0LK/gY84C4/APw1zH6tgbXuz1bucqs6iu98IMFd/mu4+CL5XfAwvkeBeyP496/0/7pX8ZVb/7/Aw9H6/Gr7ZTUSRyRTr4wAXnGXJwODRUTqIjhV3aKq89zlPcByfr7Tv6EYAUxUx2wgTUTaRyGOwcAaVd0QhXOXUdWvgZ3likN/x14BLgmz6xDgM1Xdqaq7gM9w78XyOj5Vna6qfvftbJz7uKKigs8vEnUyzVJl8bnfG1cCb9T2eaPFEokj3HQu5b+oy7Zx/zMVAm3qJLoQbpNaP+CHMKtPFZGFIvKpiBxXp4E5w7eni8hcd3qa8iL5jOvCSCr+DxzNzw/gCFXd4i5vBY4Is019+RxvwqlhhnO43wUv3ek2vY2voGmwPnx+g4CfVHV1Beuj+flViyWSBkREmgPvAr9R1d3lVs/Daa45AfgH8EEdh3e6qp6IMzPzHSJyRh2f/7Dcm1gvBt4Jszran99B1GnjqJdj80Xk9zj3d02qYJNo/S68AHQH+gJbcJqP6qOrqLw2Uu//L5VnicQRyXQuZduISALObMU76iQ655yJOElkkqq+V369qu5W1b3u8lQgUUTS6yo+Vc1zf24D3sdpQggVyWfstWHAPFX9qfyKaH9+rp9Km/vcn9vCbBPVz1FERgHDgWvcZHeICH4XPKGqP6lqQFWDwL8rOG+0P78E4DLgrYq2idbnVxOWSByRTL0yBSgdIXMF8GVF/5Fqm9um+h9guao+XcE2R5b22YjIAJx/2zpJdCLSTERalC7jdMouKbfZFOB6d/TWKUBhSDNOXanwL8Fofn4hQn/HbgA+DLPNNOB8EWnlNt2c75Z5TkSGAvcBF6vq/gq2ieR3wav4QvvcLq3gvNGeZulcYIWq5oZbGc3Pr0ai3dtfX144o4pW4Yzo+L1b9hjOfxpw5gF7B+fZKD8C3eowttNxmjkWAQvc1wXAbcBt7jZ3AktxRqHMBk6rw/i6uedd6MZQ+vmFxic4DyVbAywGsur437cZTmJIDSmL2ueHk9C2AD6cdvqbcfrcvgBWA58Drd1ts4CXQva9yf09zAFurMP4cnD6F0p/B0tHMXYAplb2u1BH8b3q/m4twkkO7cvH574/5P96XcTnlr9c+jsXsm2df361/bIpUowxxtSINW0ZY4ypEUskxhhjasQSiTHGmBqxRGKMMaZGLJEYY4ypEUskxtRD7mzEH0c7DmMiYYnEGGNMjVgiMaYGRORaEfnRfXbEv0QkXkT2isgz4jw75gsRaetu21dEZoc8z6OVW360iHzuThg5T0S6u4dvLiKT3WeATAq5836sOM+mWSQiT0Xp0o0pY4nEmGoSkWOBXwIDVbUvEACuwbmLPltVjwO+Ah5xd5kI3K+qfXDuwC4tnwQ8r86Ekafh3BENzizPvwF64dzxPFBE2uBM/3Gce5w/e3uVxhyeJRJjqm8w0B+Y4z7tbjDOF36Qnyflew04XURSgTRV/cotfwU4w51XqaOqvg+gqsX68zxWP6pqrjqTEC4AuuA8vqAY+I+IXAaEnfPKmLpkicSY6hPgFVXt6756qOqjYbar7jxEB0KWAzhPJ/TjzAY7GWcW3v9W89jG1BpLJMZU3xfAFSLSDsqeuX4Uzv+rK9xtrga+VdVCYJeIDHLLrwO+UueJl7kicol7jCYi0rSiE7rPpElVZ6r73wIneHFhxlRFQrQDMKahUtVlIvIQztPs4nBmer0D2AcMcNdtw+lHAWdq+BfdRLEWuNEtvw74l4g85h7jF5WctgXwoYgk49SIflfLl2VMldnsv8bUMhHZq6rNox2HMXXFmraMMcbUiNVIjDHG1IjVSIwxxtSIJRJjjDE1YonEGGNMjVgiMcYYUyOWSIwxxtTI/wc+pQmrkAM9bAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
